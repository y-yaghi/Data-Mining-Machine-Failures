{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d04d889-1e24-4de4-8e50-892856cc6340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T16:58:24.065236Z",
     "iopub.status.busy": "2025-04-25T16:58:24.065236Z",
     "iopub.status.idle": "2025-04-25T16:58:24.072746Z",
     "shell.execute_reply": "2025-04-25T16:58:24.072241Z",
     "shell.execute_reply.started": "2025-04-25T16:58:24.065236Z"
    }
   },
   "source": [
    "# Companion Notebook: Visualization Dashboard\n",
    "\n",
    "This notebook loads the saved artifacts from `TrainedModels/` and provides interactive visualizations:\n",
    "\n",
    "1. Feature correlation matrix\n",
    "2. 2×2 confusion matrix selector (including baselines, tuned thresholds)\n",
    "3. Decision & Random Forest tree visualizations\n",
    "4. XGBoost feature importance & threshold tuning\n",
    "5. Model performance comparison with selectable metrics and baseline inclusion via checkboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374ed4b-6588-46e1-a7d4-4f644ec7aa03",
   "metadata": {},
   "source": [
    "# 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4d77c6-e9fe-4298-8f4d-255c1d2fad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_tree\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, accuracy_score, precision_score, recall_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipywidgets import Checkbox, Dropdown, interactive_output, VBox, HBox, interact\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Load saved data\n",
    "SAVE_DIR = 'TrainedModels'\n",
    "thresh_df = pd.read_csv(os.path.join(SAVE_DIR, 'xgb_threshold_tuning.csv'))\n",
    "perf_df   = pd.read_csv(os.path.join(SAVE_DIR, 'model_performance_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ee6fc-1765-4a17-b992-66f6b1db4714",
   "metadata": {},
   "source": [
    "## 2. Data Load & Correlation Matrix\n",
    "Histogram with actual temperature difference denormalized to range °C (0–12.10 range) seen in original file, and correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0fa67-6aea-4896-8390-a41fd382176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normalized data\n",
    "dtype_dict = {\n",
    "    'norm_power':'float32',\n",
    "    'norm_temp_diff':'float32',\n",
    "    'norm_tool_wear_adjusted':'float32',\n",
    "    'Bool_MF':'bool'\n",
    "}\n",
    "data = pd.read_csv('full_normalized.csv', dtype=dtype_dict)\n",
    "data['Bool_MF_int'] = data['Bool_MF'].astype('int8')\n",
    "\n",
    "data['temp_diff_deg'] = data['norm_temp_diff'] * 12.10 # value manually calculated from source file.\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.histplot(data['temp_diff_deg'], kde=True)\n",
    "plt.title('Distribution of True Temp Difference (°C)')\n",
    "plt.xlabel('Temp Difference (°C)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Normalized correlation\n",
    "corr = data[['norm_power','norm_temp_diff','norm_tool_wear_adjusted','Bool_MF_int']].corr()\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation: Features + Failure')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare features and test split\n",
    "after = data.drop(columns=['temp_diff_deg','Bool_MF_int'])\n",
    "X = data[['norm_power','norm_temp_diff','norm_tool_wear_adjusted']]\n",
    "y = data['Bool_MF_int']\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5e62e-8af8-4f16-8b88-c4e0025b1eb4",
   "metadata": {},
   "source": [
    "## 3. Interactive 2×2 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d837bfe-57b6-4c6f-a539-a16b8b117e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    m = {}\n",
    "    # Decision Tree\n",
    "    m['DT Baseline'] = (joblib.load(os.path.join(SAVE_DIR,'DecisionTree_baseline.pkl')), None)\n",
    "    m['DT Tuned']    = (joblib.load(os.path.join(SAVE_DIR,'DecisionTree_best.pkl')), None)\n",
    "    # Random Forest\n",
    "    m['RF Baseline'] = (joblib.load(os.path.join(SAVE_DIR,'RandomForest_baseline.pkl')), None)\n",
    "    m['RF Tuned']    = (joblib.load(os.path.join(SAVE_DIR,'RandomForest_best.pkl')), None)\n",
    "    # XGBoost\n",
    "    base = xgb.Booster(); base.load_model(os.path.join(SAVE_DIR,'XGBoost_baseline.json'))\n",
    "    tuned = xgb.Booster(); tuned.load_model(os.path.join(SAVE_DIR,'XGBoost_best.json'))\n",
    "    m['XGB Baseline']     = (None, (base, 0.5))\n",
    "    for th in thresh_df['threshold']:\n",
    "        m[f\"XGB Tuned ({th:.2f})\"] = (None, (tuned, th))\n",
    "    return m\n",
    "\n",
    "models = load_models()\n",
    "labels = list(models.keys())\n",
    "\n",
    "@interact(\n",
    "    tl=Dropdown(options=labels, description='Top-left'),\n",
    "    tr=Dropdown(options=labels, description='Top-right'),\n",
    "    bl=Dropdown(options=labels, description='Bottom-left'),\n",
    "    br=Dropdown(options=labels, description='Bottom-right')\n",
    ")\n",
    "def plot_confusion_grid(tl, tr, bl, br):\n",
    "    fig, axes = plt.subplots(2,2, figsize=(12,10))\n",
    "    for ax, name in zip(axes.flatten(), [tl, tr, bl, br]):\n",
    "        clf, info = models[name]\n",
    "        if clf:\n",
    "            preds = clf.predict(X_test)\n",
    "        else:\n",
    "            bst, thr = info\n",
    "            proba = bst.predict(xgb.DMatrix(X_test))\n",
    "            preds = (proba > thr).astype(int)\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax, cbar=False)\n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c045d9-adae-487c-9983-33165bde48bf",
   "metadata": {},
   "source": [
    "## 4. Decision & Random Forest Tree Visualizations\n",
    "This section lets you inspect the structure of the first few levels of a Decision Tree or Random Forest (single tree).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd0f0b-8d35-430c-a27a-f3cac9045404",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(model=Dropdown(options=['DT Baseline','DT Tuned','RF Baseline','RF Tuned'],\n",
    "                         description='Model'))\n",
    "def plot_sklearn_trees(model):\n",
    "    clf, _ = models[model]\n",
    "    if model.startswith('RF'):\n",
    "        clf = clf.estimators_[0]\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plot_tree(clf,\n",
    "              feature_names=['norm_power','norm_temp_diff','norm_tool_wear_adjusted'],\n",
    "              class_names=['0','1'], filled=True, max_depth=3)\n",
    "    plt.title(f\"{model} (first tree, max depth=3)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c2524-6841-4e2c-acc7-b897062cf7cb",
   "metadata": {},
   "source": [
    "## 5. XGBoost Feature Importance & Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77186ba-3d91-4d32-89a4-8ac7bdff87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "bst = xgb.Booster(); bst.load_model(os.path.join(SAVE_DIR,'XGBoost_best.json'))\n",
    "xgb.plot_importance(bst, importance_type='weight')\n",
    "plt.title('XGB Feature Importance (F-score)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if 'f1' not in thresh_df.columns:\n",
    "    thresh_df['f1'] = 2 * (thresh_df['precision'] * thresh_df['recall']) / (thresh_df['precision'] + thresh_df['recall'])\n",
    "plt.figure(figsize=(8,5))\n",
    "for col in ['accuracy','precision','recall','f1']:\n",
    "    plt.plot(thresh_df['threshold'], thresh_df[col], marker='o', label=col)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('XGB Threshold Tuning')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5706a9c-ce48-4443-83d4-9880ce6218fb",
   "metadata": {},
   "source": [
    "## 6. Model Performance Comparison\n",
    "\n",
    " Compute train/test metrics for core models, including baseline XGB & tuned thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586acc3a-18bf-41be-81ee-621fc4125c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model dict\n",
    "comp_models = {\n",
    "    'DT Baseline': models['DT Baseline'][0],\n",
    "    'DT Tuned':    models['DT Tuned'][0],\n",
    "    'RF Baseline': models['RF Baseline'][0],\n",
    "    'RF Tuned':    models['RF Tuned'][0],\n",
    "    'XGB Baseline':('xgb', models['XGB Baseline'][1]),\n",
    "    'XGB Tuned (0.50)':('xgb', models['XGB Tuned (0.50)'][1])\n",
    "}\n",
    "th = [0.10,0.15,0.20,0.25,0.30]\n",
    "for t in th:\n",
    "    comp_models[f\"XGB Tuned ({t:.2f})\"] = models[f\"XGB Tuned ({t:.2f})\"]\n",
    "\n",
    "# Widgets\n",
    "metric_dd = Dropdown(options=['Accuracy','Precision','Recall','F1'], description='Metric')\n",
    "# Default: baselines + 0.10,0.20,0.30 thresholds\n",
    "defaults = [name for name in comp_models if 'Baseline' in name or any(name.endswith(f\"({d:.2f})\") for d in [0.10,0.20,0.30])]\n",
    "model_checkboxes = {name: Checkbox(value=(name in defaults), description=name)\n",
    "                    for name in comp_models}\n",
    "# Arrange in 3 columns\n",
    "from math import ceil\n",
    "names = list(comp_models)\n",
    "cols = ceil(len(names)/3)\n",
    "groups = [names[i*cols:(i+1)*cols] for i in range(3)]\n",
    "col_widgets = [VBox([model_checkboxes[n] for n in grp]) for grp in groups]\n",
    "ui = VBox([metric_dd, HBox(col_widgets)])\n",
    "\n",
    "# Plot function\n",
    "# %%\n",
    "def plot_selected(metric, **checks):\n",
    "    sel = [n for n, v in checks.items() if v]\n",
    "    rows = []\n",
    "    for name in sel:\n",
    "        item = comp_models[name]\n",
    "        for split,Xd,yd in [('Train',X,y),('Test',X_test,y_test)]:\n",
    "            if isinstance(item, tuple):\n",
    "                bst, thr = item[1]\n",
    "                proba = bst.predict(xgb.DMatrix(Xd))\n",
    "                preds = (proba>thr).astype(int)\n",
    "            else:\n",
    "                preds = item.predict(Xd)\n",
    "            acc  = accuracy_score(yd,preds)\n",
    "            prec = precision_score(yd,preds)\n",
    "            rec  = recall_score(yd,preds)\n",
    "            f1   = 2*(prec*rec)/(prec+rec) if (prec+rec)>0 else 0\n",
    "            rows.append({'Model':name,'Split':split,\n",
    "                         'Accuracy':acc,'Precision':prec,'Recall':rec,'F1':f1})\n",
    "    df = pd.DataFrame(rows)\n",
    "    pivot = df.pivot(index='Model', columns='Split', values=[metric])\n",
    "    pivot.columns = [f\"{metric} {s}\" for (_,s) in pivot.columns]\n",
    "    pivot['Diff'] = pivot[f\"{metric} Train\"]-pivot[f\"{metric} Test\"]\n",
    "    plot_df = pivot.reset_index().melt(id_vars='Model',\n",
    "        value_vars=[f\"{metric} Train\",f\"{metric} Test\"], var_name='Split', value_name=metric)\n",
    "    colors = sns.color_palette('muted')[:2]\n",
    "    ax = sns.barplot(x=metric,y='Model',hue='Split',data=plot_df,\n",
    "                     palette=colors,orient='h')\n",
    "    plt.title(f\"{metric}: Train vs Test (Diff shown)\")\n",
    "    xmin,xmax = plot_df[metric].min()-0.01, plot_df[metric].max()+0.01\n",
    "    for idx,row in pivot.reset_index().iterrows():\n",
    "        ax.text(xmin+0.02*(xmax-xmin), idx, f\"{row['Diff']:.4f}\",\n",
    "                va='center', ha='left', bbox=dict(boxstyle='round,pad=0.2',facecolor='yellow',alpha=0.6))\n",
    "    ax.get_legend().remove()\n",
    "    plt.xlim(xmin,xmax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# display main comparison\n",
    "# %%\n",
    "out = interactive_output(plot_selected, {'metric':metric_dd, **model_checkboxes})\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfdf1d-a5de-4704-8d8c-3614c632cebb",
   "metadata": {},
   "source": [
    "### 6b. Metric Differences Only\n",
    "Select models above and choose ordering to view only the train-test differences for all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1f70f-f526-4753-8c26-a8da978f4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by metric for diffs\n",
    "order_dd = Dropdown(options=['Accuracy','Precision','Recall','F1'], description='Metric (Diff)')\n",
    "\n",
    "# %%\n",
    "def plot_diff_all(order_by, **checks):\n",
    "    # Gather train-test diff for all metrics across selected models\n",
    "    sel = [n for n,v in checks.items() if v]\n",
    "    rows = []\n",
    "    for name in sel:\n",
    "        item = comp_models[name]\n",
    "        # compute metrics for train/test\n",
    "        mvals = {}\n",
    "        for split,Xd,yd in [('Train',X,y),('Test',X_test,y_test)]:\n",
    "            if isinstance(item, tuple):\n",
    "                bst,thr = item[1]\n",
    "                proba = bst.predict(xgb.DMatrix(Xd))\n",
    "                preds = (proba>thr).astype(int)\n",
    "            else:\n",
    "                preds = item.predict(Xd)\n",
    "            mvals[split] = {\n",
    "                'Accuracy': accuracy_score(yd,preds),\n",
    "                'Precision': precision_score(yd,preds),\n",
    "                'Recall': recall_score(yd,preds),\n",
    "                'F1':  2*(precision_score(yd,preds)*recall_score(yd,preds))/(precision_score(yd,preds)+recall_score(yd,preds)) if (precision_score(yd,preds)+recall_score(yd,preds))>0 else 0\n",
    "            }\n",
    "        # record diffs for each metric\n",
    "        for metric in ['Accuracy','Precision','Recall','F1']:\n",
    "            diff = mvals['Train'][metric] - mvals['Test'][metric]\n",
    "            rows.append({'Model':name,'Metric':metric,'Diff':diff})\n",
    "    df_diff = pd.DataFrame(rows)\n",
    "    # determine model order based on selected metric\n",
    "    order_df = df_diff[df_diff['Metric']==order_by].sort_values('Diff')\n",
    "    order_list = order_df['Model'].tolist()\n",
    "    # plot all metrics per model, sorted\n",
    "    plt.figure(figsize=(8, max(4, len(order_list)*0.5)))\n",
    "    ax = sns.barplot(x='Diff', y='Model', hue='Metric', data=df_diff, orient='h', order=order_list)\n",
    "    plt.title(f'Train vs Test Differences Ordered by {order_by}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# display diff-only plot with ordering\n",
    "tout = interactive_output(plot_diff_all, {'order_by':order_dd, **model_checkboxes})\n",
    "display(VBox([HBox(col_widgets), order_dd]), tout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4bca0-1b54-44fa-aedf-72795cc41060",
   "metadata": {},
   "source": [
    "## 7. SHAP Explanation for XGB Tuned (0.20)\n",
    "In this section, we use SHAP to explain the feature contributions for the XGBoost model tuned at 0.20 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4edc49-3ef5-4a5d-a558-44909d81bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colorbar as mcolorbar  # for custom colorbar\n",
    "import shap\n",
    "# helper to recreate and resize SHAP colorbars\n",
    "def recreate_shap_colorbar(fig, width_multiplier=0.5, main_ax_index=0, collection_index=-1):\n",
    "    main_ax = fig.axes[main_ax_index]\n",
    "    if not main_ax.collections:\n",
    "        print(f\"Warning: no collections on axis {main_ax_index}\")\n",
    "        return\n",
    "    col = main_ax.collections[collection_index]\n",
    "    cmap, norm = col.get_cmap(), col.norm\n",
    "    # remove existing colorbar\n",
    "    cbar_ax = fig.axes[-1]\n",
    "    label = cbar_ax.get_ylabel()\n",
    "    cbar_ax.remove()\n",
    "    # compute new position\n",
    "    pos = cbar_ax.get_position()\n",
    "    new_width = pos.width * width_multiplier\n",
    "    new_x0 = pos.x0 - (new_width - pos.width)\n",
    "    rect = [new_x0, pos.y0, new_width, pos.height]\n",
    "    # create new colorbar\n",
    "    new_ax = fig.add_axes(rect)\n",
    "    cb = mcolorbar.ColorbarBase(new_ax, cmap=cmap, norm=norm, orientation='vertical')\n",
    "    cb.set_label(label)\n",
    "\n",
    "# Extract tuned XGBoost booster and threshold\n",
    "_, (xgb_tuned, thr_020) = models['XGB Tuned (0.20)']\n",
    "\n",
    "# Create explainer and sample data with 500 rows for speed\n",
    "explainer = shap.TreeExplainer(xgb_tuned)\n",
    "X_sample = X_test.sample(n=500, random_state=42)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# -- Summary (beeswarm) plot --\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=X_sample.columns, show=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 6)\n",
    "# resize colorbar\n",
    "destroy = recreate_shap_colorbar(fig)\n",
    "plt.show()\n",
    "\n",
    "# -- Dependence plots for each feature --\n",
    "for feat in X_sample.columns:\n",
    "    shap.dependence_plot(feat, shap_values, X_sample, show=False)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 6)\n",
    "    recreate_shap_colorbar(fig, main_ax_index=0, collection_index=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d44eb-b6c4-4527-b1a8-7f55e89bedd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
